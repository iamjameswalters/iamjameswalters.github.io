Title: Robots Aren't Responsible
Date: 2024-02-08
Author: James Walters
Category: Meta
Tags: Artificial intelligence, ethics

DHH posted a recent piece called [It's easier to forgive a human than a robot](https://world.hey.com/dhh/it-s-easier-to-forgive-a-human-than-a-robot-d4a97b3a). In it, he highlights a fascinating problem and draws a handful of conclusions that I think head off in the wrong directions.

The problem he highlights is that as AI and similar systems take over more dangerous tasks, the responsibility for mistakes shifts from humans to the systems. His main example is driving. As Tesla introduces better self-driving technology in more vehicles, DHH posits a scenario where the 40,000+ deaths due to auto accidents in the US are halved because of self-driving cars, but that leaves Tesla on the hook for 10,000 deaths. To quote him:

> Because let’s just say you’re Tesla. And suddenly half of everyone in America is being driven by one of your robo-cars. Your self-driving tech is highly advanced. 50% better than humans! That leaves you responsible for 10,000 deaths per year. Eeks! Okay, let’s say you’re another order of magnitude better, that’s still 1,000 deaths. Two orders? 100 deaths. Per year.
>
> The math is easy, the human element is hard, and the legal ramifications perhaps impossible.

He goes on to muse about the psychological palatability of impersonal computer systems bearing blame for the deaths of thousands of people, and suggests that "maybe this is just a phase," that we'll develop mental categories for forgiving machines.

I think David accidentally gives away the real nature of the problem. In his example, he doesn't say, "that leaves the self-driving system responsible for 10,000 deaths per year," he says, "that leaves _you_ responsible." We wouldn't hold the computer, the self-driving car, responsible for mistake. We would hold Tesla responsible.

## The Nature of Blame

This is one of those instances where despite the heaps of fog and shadow that are cast over an intellectual problem, we know the real answer within our own skin, as a gut instinct. The machine is not responsible because the machine _cannot_ be responsible.

With issues like killing people with cars, or medical malpractice by AI doctors, or [large language models lying](https://simonwillison.net/2023/Apr/7/chatgpt-lies/), these are not issues that are morally neutral. When people die, when the truth is distorted, these are moral issues. And moral issues demand moral blame. Moral blame requires volition. Machines are incapable of volition&mdash;only their makers are.

To suggest that systems designed by human beings are released from the chain of moral causation and are transmuted into brute, mechanistic forces of chance such as those in the realm of nature is, at least, a controversial claim to make. I am not persuaded that the echos of human intentionality and accountability are washed away so soon after the Teslas leave the factory.

## It's still the Chinese Room Argument

Conversations like this&mdash;and there are many of them these days&mdash;are encumbered by playing fast and loose with terms like "artificial intelligence." The truth is that we're training computers to be pretty decent at extremely specific tasks. AI systems have no real cognition, no model of consciousness. No volition, no will; just a program that is, ultimately, trained to guess the most likely response to a given prompt, even if that prompt is really complex compared to anything that's been possible before.

It's still the [chinese room argument](https://en.wikipedia.org/wiki/Chinese_room).

What's complicated about all of this, of course, is that whereas traditionally you could draw a one-to-one relationship between the will of the programmer and output of the program, this is no longer possible as we enter the world of AI systems whose inner machinations are too complex for us to fully understand, and behave in ways that may be predictable but not wholly deterministic. This is simply the nature of these systems. 

We're going to have to decide how much consistency we're going to demand from indeterminate systems like AI models before we hold the companies who develop them accountable. 